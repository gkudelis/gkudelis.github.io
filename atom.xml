<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>Blog of Giedrius Kudelis</title>
    <link href="https://gkudelis.net/atom.xml" rel="self" />
    <link href="https://gkudelis.net" />
    <id>https://gkudelis.net/atom.xml</id>
    <author>
        <name>Giedrius Kudelis</name>
        <email>giedrius.kudelis@gmail.com</email>
    </author>
    <updated>2019-05-09T00:00:00Z</updated>
    <entry>
    <title>Developing for Pebble using Docker</title>
    <link href="https://gkudelis.net/posts/2019-05-09-developing-for-pebble-using-docker.html" />
    <id>https://gkudelis.net/posts/2019-05-09-developing-for-pebble-using-docker.html</id>
    <published>2019-05-09T00:00:00Z</published>
    <updated>2019-05-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Developing for Pebble using Docker</h2>

    <p class="info">
        Published:  9 May, 2019
        
        <br>Tags: <a href="/tags/pebble.html">pebble</a>, <a href="/tags/docker.html">docker</a>
    </p>

    <p>A while ago I’ve come up with an idea for a watch. I thought it would benefit from custom hardware, but what I really wanted to do is to quickly make a working prototype and validate it. I looked around for hardware that would be easy to code for and have all the functions I needed and quickly settled on the Pebble watch. I found a couple (well, my very generous colleague donated them to me) and started looking into how to get the toolchain working and put my ideas to test.</p>
<h3 id="connecting-to-the-pebble">Connecting to the Pebble</h3>
<p>The main Pebble developer support site that comes up now is called <a href="https://developer.rebble.io/developer.pebble.com/index.html">Pebble Developers</a>. You can find a lot of helpful content there including <a href="https://developer.rebble.io/developer.pebble.com/sdk/install/index.html">how to get the SDK set up and running</a>. I tried following it on my Mac, but just couldn’t get it working. Now, this is exactly the kind of problem that Docker should be able to solve for me! I went looking for an image that would have the Pebble SDK and found <a href="https://hub.docker.com/r/andredumas/pebble-dev">andredumas/pebble-deb</a>.</p>
<p>The next thing I had to do was find a way to connect the Pebble to my Mac. I’d assumed (wrongly) that the USB cable used to charge it would serve that purpose, but the way to send programs to the watch is via Bluetooth. Reading <a href="https://developer.rebble.io/developer.pebble.com/guides/tools-and-resources/developer-connection/index.html">the pebble cli tool documentation</a> reveals that you need to either connect the Pebble directly to your Mac using Bluetooth at which point it should appear as a device inside <code>/dev/</code> or you can use Developer Connection available through the Android and iOS apps.</p>
<p>I personally would’ve preferred the former approach as I don’t enjoy faffing about with phone apps when developing, but since I couldn’t get the SDK working on my machine and the Docker is running in a VM (so I couldn’t easily pass the device into the container) I ended up going down the app route. Be warned - the app is now discontinued and has been removed from the app store, so now you have to download it from <a href="https://play.google.com/store/apps/details?id=hu.czandor.pebblerebblehelper">APKMirror</a>. Not sure what exactly you need to do to get it working with an iPhone, but if you email me I’ll be glad to add instructions for it to this article (or link to yours).</p>
<p>Once you download the app to your phone you can install it from the APK file. Launch the app, skip the login and you should have the app working. Next you need to connect your pebble - click on the refresh icon in the top-right corner, select the kind of Pebble you have and click on your device in the list. You then have to go through the Bluetooth pairing procedure after which the app might decide your watch needs an update. Once that’s done and you have connected your Pebble to the app you’ll need to follow the directions for <a href="https://developer.rebble.io/developer.pebble.com/guides/tools-and-resources/developer-connection/index.html">turning on Developer Connection</a>.</p>
<h3 id="building-and-installing-an-app">Building and installing an app</h3>
<p>Now that the Developer connection is on let’s take an example app and install it on the watch. A nice example is <a href="https://github.com/pebble-examples/simple-analog/">the simple-analog watchface</a>. To clone it run</p>
<pre><code>git clone https://github.com/pebble-examples/simple-analog.git</code></pre>
<p>Enter the project folder, then build the app:</p>
<pre><code>docker run --rm -it -v $PWD:/pebble andredumas/pebble-dev build</code></pre>
<p>If all went well (I get some Python errors, but the build still gets completed) take the IP shown in Developer Connection and install the app on your Pebble:</p>
<pre><code>docker run --rm -it -v $PWD:/pebble andredumas/pebble-dev install --phone=192.168.0.1</code></pre>
<p>This will install the watchface on your Pebble and it should become the current watchface. You can follow exactly the same process to build and install apps.</p>
<h3 id="alternative-connection-options">Alternative connection options</h3>
<p>I have to admit that I don’t love having to download and install an app from a mirror just to use the phone as a WiFi-Bluetooth bridge. I tried setting up a Bluetooth serial connection to the Pebble from a Linux machine (hoping I can use the <code>--device</code> option to <code>docker run</code> to pass it through to the container), but it just kept disconnecting and I gave up. If you know how to get it working - please let me know!</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Tock notifications in macOS</title>
    <link href="https://gkudelis.net/posts/2019-04-03-tock-notifications-in-macos.html" />
    <id>https://gkudelis.net/posts/2019-04-03-tock-notifications-in-macos.html</id>
    <published>2019-04-03T00:00:00Z</published>
    <updated>2019-04-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Tock notifications in macOS</h2>

    <p class="info">
        Published:  3 Apr, 2019
        
        <br>Tags: <a href="/tags/other.html">other</a>
    </p>

    <p>Quite a while ago I’ve come across the concept of tocks on the <a href="https://blog.beeminder.com/tocks/">Beeminder Blog</a> and started using them when I work. The general idea is that you work in 45 minute uninterrupted chunks and count the number of these you do in a day. I’ve noticed it works well for me and stuck with it.</p>
<p>I was going about it in quite a lazy way tool-wise - I’d simply find a timer that would run in a browser tab, set it to 45 minutes and use that. However, the only way for it to notify me is if I set an audible alarm. Recently I found myself at a library without headphones and recognizing not everyone might appreciate the loud alarm going off.</p>
<p>I’m on a Mac, so an alert that applications can show in the upper right corner seemed like a natural solution - I looked for a CLI tool to display alerts and found <a href="https://github.com/julienXX/terminal-notifier">terminal-notifier</a>. It works great, but it shows a message immediately when you call it and I needed a timeout. I also quite like to be able to check how much time I have left in the current tock and be able to pause it when needed, so it would be nice to do something more clever than <code>sleep 2700</code>. A great solution is using another tool called <a href="https://github.com/trehn/termdown">termdown</a>, which displays a nice ASCII art countdown in your terminal window and lets you pause it using the space key. You can also add a suffix to signify hours or minutes. I am calling it as</p>
<pre><code>termdown 45m</code></pre>
<p>When the time runs out <code>termdown</code> simply exits, so I was combining it with <code>terminal-notifier</code> like this:</p>
<pre><code>termdown 45m &amp;&amp; terminal-notifier -title &quot;Tock&quot; -message &quot;Your tock is over&quot;</code></pre>
<p>This is great, but the alert simply disappears after a couple of seconds. What if I miss it? After all, sometimes when I’m not typing or reading I’m not just staring at the screen… I wanted the alert to have buttons and stick around until I explicitly close it, however, <code>terminal-notifier</code> doesn’t let you do that. In the README it suggests using <a href="https://github.com/vjeantet/alerter">alerter</a> if you want this kind of functionality. Calling <code>alerter</code> with the same arguments as <code>terminal-notifier</code> gives me exactly what I want and now I’m using this:</p>
<pre><code>termdown 45m &amp;&amp; alerter -title &quot;Tock&quot; -message &quot;Your tock is over&quot;</code></pre>
<p>This works well, gives me a nice countdown and I’m sure to notice the alert as it sticks around until I close it. No need to disturb my library neighbors!</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Lazy prime number generator in Python</title>
    <link href="https://gkudelis.net/posts/2019-02-27-prime-number-generator.html" />
    <id>https://gkudelis.net/posts/2019-02-27-prime-number-generator.html</id>
    <published>2019-02-27T00:00:00Z</published>
    <updated>2019-02-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Lazy prime number generator in Python</h2>

    <p class="info">
        Published: 27 Feb, 2019
        
        <br>Tags: <a href="/tags/python.html">python</a>, <a href="/tags/generators.html">generators</a>
    </p>

    <p>A while ago I read <a href="https://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf">The Genuine Sieve of Eratosthenes</a> paper by <a href="https://www.cs.hmc.edu/~oneill/">Melissa E. O’Neill</a> where the author explains how the algorithm that is often portrayed in introductory texts on functional programming as the lazy functional implementation of Sieve of Eratosthenes is actually not the sieve and is actually even worse than trial division (testing numbers by checking if any prime already found divides them). She then outlines and implements an algorithm that is actually a lazy and functional sieve and uses that to show how important the choice of a suitable datastructure can be for performance.</p>
<p>After reading the paper I wrote a quick implementation of the algorithm in Clojure and forgot about it. I recently had to do some data processing at work. I had to load some data, do some transformations and save it for later analysis (classic ETL). I wasn’t sure how much data there would be and so I decided it’s best to write my script in a lazy manner, so it wouldn’t have to fetch all the data before starting to process it. I was doing this in Python and so had a chance to explore Python generators a little more in-depth (I’ve somehow managed to ignore this really neat part of Python up until then) and as I learned more about generator expressions and functions I realised that the lazy algorithm for generating prime numbers can be expressed really easily and clearly using generator functions.</p>
<p>In this post I want to quickly run through the basics of generator functions in Python and explain how they can be used to create lazy sequences. Then I’ll explain how the trial division algorithm works and use it to create a generator function for prime numbers. And lastly I’ll go through the prime number algorithm described by Prof. O’Neill and show how to implement it in Python.</p>
<p>I also want to thank my friend <a href="https://bram.xyz/blog/">Bram Geron</a> for his input and help while I was experimenting with some of the different possible implemenetations, working through the differences in complexity and learning about the different tools out there. At least for me this sort of process is way more fun and rewarding when you can share it with others and bounce ideas off each other.</p>
<h3 id="generator-functions">Generator functions</h3>
<p>In Python generator functions are one of the ways to create iterators, which is to say objects that have a <code>__next__</code> method. When you call the Python inbuilt <code>next</code> function and pass it an iterator it in turn calls the <code>__next__</code> method, which returs the next item in the iterator. This is done implicitly in <code>for</code> loops and many other places in Python, so you don’t often have to use <code>next</code> in your own code. Mind you this is a bit of a simplification and if you’d like a full view of the differences between iterables, iterators, and other related interfaces you might want to check out <a href="https://nvie.com/posts/iterators-vs-generators/">Iterables vs. Iterators vs. Generators</a> by Vincent Driessen.</p>
<p>Generator functions offer a nice and flexible way for creating iterators without the boilerplate of making one by creating a class. Instead, a generator looks exactly like a function, except it has one or more <code>yield</code> statements. When you call this function a generator object is returned (generators are also iterators), but the function is not executed. Instead, once <code>next</code> is called on the generator object the function actually runs. Once the execution reaches a <code>yield</code> statement the argument is returned as the first item of the iterator and execution is paused. The next time <code>next</code> is called the execution resumes from the same point until the next <code>yield</code> statement is reached. When the generator is finished it can either use an empty <code>return</code> statement or the function can simply finish without one.</p>
<p>Let’s write a generator that describes the Fibonacci sequence as a a simple example. We first set up two variables to contain the two previous items of the sequence and set them both to 1 as we know those are the first two items. We can emit those immediately after and then enter an infinite loop where we calculate the next item by adding the two previous items, emit the new item and then update our values for the two previous items of the sequence.</p>
<pre><code>def fibonacci():
    a = 1
    b = 1
    yield a
    yield b
    while True:
        c = a + b
        yield c
        a = b
        b = c</code></pre>
<p>We can then use it to create a generator object, get the first 10 items using <code>itertools.islice</code> and print them.</p>
<pre><code>from itertools import islice

fs = fibonacci()
for f in islice(fs, 10):
    print(f)</code></pre>
<p>There is more to say about generator functions in Python, including <code>yield from</code> and sending values back to the generator using the <code>.send()</code> method, but this will be enough to cover the basics required for the rest of the article.</p>
<h3 id="trial-division">Trial division</h3>
<p>Moving on to actually generating some prime numbers - trial division is an algorithm that more or less follows from the definition of prime numbers. You take a candidate number and check how many numbers divide into it. Of course, you can make some optimizations. First of all, if you find a number (other than 1 or itself) that divides into it you already know it’s not a prime number, so you don’t have to continue and instead move on to the next candidate. Also, if you have already found all the prime numbers smaller than the current candidate then it’s much faster to only check the current candidate for prime factors. Lastly, there’s no need to check factors larger than the square root of the candidate - for every factor greater than the square root there will be one that is less.</p>
<pre><code>from itertools import count

def primes_trial_division():
    primes = []
    for candidate in count(2):
        factor_found = False
        for p in primes:
            if candidate % p == 0:
                factor_found = True
                break
            elif p * p &gt; candidate:
                break
        if not factor_found:
            yield candidate
            primes.append(candidate)</code></pre>
<p>Here we start with an empty list of primes and using <code>itertools.count</code> start counting up starting with 2. We could add 2 to the initial list of primes and count up from 3 with a step of 2 and it would create a performance boost. However, it’s a constant factor improvement and does not affect the time complexity. If you are interested in making this more performant you should go through the paper linked at the top of this post - the author goes through a generalization of the stepping technique called a wheel that can be used to further improve constant factors.</p>
<p>For every candidate we iterate over the list of primes we have already found. If we find that a prime from our list divides into the current candidate we note that we have found a factor and break the loop. In case we find that we’ve started checking primes that are larger than the square root of the candidate we break as well - if we haven’t come across a factor by then there’s no point iterating through the rest of the prime list.</p>
<p>Finally, if we see that we haven’t found a prime factor of our candidate we know that the candidate is in fact a prime number, so we <code>yield</code> it and append to the list of known primes. Since <code>itertools.count</code> produces an infinite sequence this generator function will produce and infinite sequence of prime numbers as well.</p>
<p>If you’re interested in the time complexity there is a great discussion in the paper linked to at the top of the post. For trial division it turns out to be \(O(n \sqrt{n} / (\log n)^2)\), where \(n\) is the number of candidates we want to test.</p>
<h3 id="lazy-sieve">Lazy sieve</h3>
<p>To implement a lazy sieve as described by Prof. O’Neill we use the general idea of the classic sieve of Eratosthenes algorithm, but whereas the classic sieve removes all multiples of all the primes already discovered from a given starting set the lazy sieve tries to do as little work upfront as possible. The algorithm keeps a map where the values are (maybe empty) lists of prime numbers and the key of such list is the number that is the next one divisible by the primes in the list. In other words, it maps from numbers to their prime factors. This map starts out empty as we don’t know any prime numbers yet.</p>
<pre><code>from itertools import count

def primes_lazy_sieve():
    multiples = {}
    for candidate in count(2):
        candidate_divisors = multiples.pop(candidate, None)
        if candidate_divisors is None:
            yield candidate
            multiples[candidate * candidate] = [candidate]
        else:
            for divisor in candidate_divisors:
                multiples.setdefault(candidate + divisor, []).append(divisor)</code></pre>
<p>We again generate our candidates by counting up from 2 and we look up the candidate in the map. If the map does not contain the candidate (as in there is no such key) it means that it is a prime number (it does not have any prime factors other than itself) - in that case we <code>yield</code> it and then add it to the list of known prime factors for candidate times 2 (in the code above it is instead candidate squared, I will explain this later). If the map contains the candidate it means it’s instead a composite number. We remove the record from the map, but the prime divisors get “propagated” up to their next multiple, which is the candidate plus the prime. This way every prime we’ve found is only ever in one of the lists at any given time, always assigned to the key that is the next multiple of that prime (next with respect to the current candidate).</p>
<p>The reason it is possible to use candidate squared instead of candidate times 2 is because the candidate times 2 case will already be eliminated since 2 is one of its prime factors. The first composite number that would not be eliminated by smaller prime factors is candidate squared, so we instead start there.</p>
<p>In her paper Prof. O’Neill the importance of choosing the right data structure for the job and shows that using a priority queue yields better time complexity. This makes sense - in Haskell the <code>Data.Map</code> datastructure is <a href="http://hackage.haskell.org/package/containers-0.6.0.1/docs/Data-Map.html">based on binary trees</a> and so every access incurs an \(O(\log n)\) cost, whereas a priority queue offers \(O(1)\) access to the first item. In this case it’s the item with the lowest priority and the only one we need to check to figure out if our candidate has any factors. This change of datastructure creates a significant performance difference.</p>
<p>Before I started writing this post I tried to implement this suggestion and speed up the lazy sieve above using a Python priority queue library called <code>heapq</code>. To my surprise the complexity increased quite dramatically. After a little investigation into why I’m having such different results I looked up Haskell’s <code>Data.Map</code> and realized it’s based on binary trees while Python dictionaries are hash tables and thus have \(O(1)\) time complexity for getting, setting and deleting items. Since these are the only operations I’m using there’s not much that can be done to improve complexity on this front. The hashing function that Python uses for positive integers is simply identity, so there’s not much performance to be gained there either.</p>
<p>Looking at the time complexity of the lazy sieve we see that every candidate regardless of whether or not it’s a prime number will incur a pop from the map, which means there is an \(O(n)\) component when looking for all primes less than \(n\). If the candidate is a prime we add it to the map and if it’s a composite we iterate over all it’s prime factors and add them to the map. Instead of thinking about counting the number of factors of composites we can consider that every prime candidate will be added to the map and then moved \(n / p\) times (starting at candidate squared means this number is lower, but that’s not significant for the time complexity calculation) and each of those operations is constant in time. This leads to the same complexity calculation as you would have for the classic sieve of Eratosthenes, which means that the complexity of the lazy sieve is \(O(n \log \log n)\). If you’re interested in the fine details I refer again to the paper by Prof. O’Neill where she derives this result.</p>
<h3 id="the-bigger-picture">The bigger picture</h3>
<p>My intention when writing this post was not only to cover basic syntax of Python generator functions and run through the lazy sieve algorithm, but really to show how generator functions can give us a way to express certain lazy algorithms in a very readable and concise way. I know I personally find them much easier to follow than the same thing expressed as a class implementing the iterable interface. In a lot of ways it’s about knowing what variables are in scope and who else has access to them. In the case of a generator expression it’s pretty obvious - regular function scoping rules apply, the variables are local and no other code has access to them. In an iterable object the answer is “it depends”.</p>
<p>Even if you don’t see any use for generator functions in the code you’re writing at the moment I’d suggest having a play with them to get a sense of how they work and how to use them. At the very least it will show you another way of looking at problems and provide a new way to structure your code.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Using entr and Docker together for development</title>
    <link href="https://gkudelis.net/posts/2019-01-23-entr-and-docker.html" />
    <id>https://gkudelis.net/posts/2019-01-23-entr-and-docker.html</id>
    <published>2019-01-23T00:00:00Z</published>
    <updated>28 Jan, 2019</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Using entr and Docker together for development</h2>

    <p class="info">
        Published: 23 Jan, 2019
        
            <br>Updated: 28 Jan, 2019
        
        <br>Tags: <a href="/tags/ag.html">ag</a>, <a href="/tags/entr.html">entr</a>, <a href="/tags/docker.html">docker</a>
    </p>

    <p>Lately I’ve found myself using Docker more and more. There’s something quite intoxicating about quick and easy creation of identical environments and the portability that comes from it. However, for me that often looks as vim in one tmux pane and a shell in another where I run something like</p>
<pre><code>docker run --rm -it $(docker build -q .)</code></pre>
<p>then look at the running application, change something, switch back into the shell window, kill Docker, run same command again. This is pretty tedious and there’s no reason it can’t be automated.</p>
<p>Up until recently the only way to achieve this I knew of was the inotify system and the incron service, which makes it easier to manage, especially if you’re familiar to setting up cron events. However, I’ve resently come across <a href="http://eradman.com/entrproject/">a tool called entr</a> that helps achieve a similar result, but without depending on the inotify system, making it a cross-platform solution. In simple terms what entr does is it takes a list of filenames via standard input and a command to run as a command line argument. When it notices that any of the files has changed it runs the given command. For example, if you’re working on a Python project and you want to clear the screen (passing the <code>-c</code> flag to entr makes it clear the screen) and re-run tests any time you make changes you might run it as</p>
<pre><code>ag -l | entr -c python -m unittest</code></pre>
<p>Here, ag is responsible for finding interesting files for entr to watch. You could use find, but ag pays attention to your .gitignore and has some other useful defaults. Note however that this wouldn’t detect and start tracking files created after starting the command. This can be changed by using the <code>-d</code> flag of entr, which makes entr watch any directories containing the files it’s tracking and exit when it detects there are new files. This is intentional as it’s not the responsibility of entr to figure out which files it should track. The intended way to use this functionality is by wrapping both commands in a shell while-true loop, this way the file listing command is used to tell entr which of the new files it’s supposed to be tracking.</p>
<pre><code>while sleep 1; do
ag -l | entr -cd python -m unittest
done</code></pre>
<p>So can we use a similar setup to build and run a Docker container? Of course! If your container runs some tests and exits or does some other task that quickly finishes the only change you have to do is wrap the <code>docker</code> command in an <code>sh -c</code> or if you’re using an up to date version of entr you can use the <code>-s</code> flag which effectively does the same (my distribution shipped version 3.4 and the <code>-s</code> flag was not available).</p>
<pre><code>while sleep 1; do
ag -l | entr -cds &#39;docker run --rm $(docker build -q .)&#39;
done</code></pre>
<p>Note that whether you’re using the <code>-s</code> flag or the <code>sh -c</code> technique the command now needs to be quoted to avoid premature shell variable expansion.</p>
<p>However, if you plan to run a development server this way the workflow is a little different - now entr should ask the Docker container to terminate, wait until that’s done and only then build and run a new container. Luckily, entr has the <code>-r</code> flag that it send a signal to the proccess it started, wait for the process to terminate and only then start a new one. Though if you simply add the flag to the above command you’ll run into issues - the container will refuse to stop. It’s not an issue with entr or Docker, it’s simply an outcome of how the Linux kernel treats signals sent to processes. For regular processes if you don’t supply a signal handler you basically get a default one assigned and as a developer you don’t need to worry about correctly handling all the different signals your application might receive. Normally these default signal handlers do the right thing. However, the way the process with PID 1 is treated is a little different - the default handlers for the “please exit” signals don’t actually terminate the process. If you want your process to listen to these signals and exit you have to implement these handlers yourself. Of course, that’s a bit of a hassle, so since Docker version 1.13 it ships with a small and simple init manager (originally called tini) that can be used to wrap the application so that the init manager runs as PID 1 and executes your application as a different PID. It also knows how to handle different signals, making it behave as entr would expect. To run your application in a container using the init manager you can use</p>
<pre><code>while sleep 1; do
ag -l | entr -cdrs &#39;docker run --rm --init $(docker build -q .)&#39;
done</code></pre>
<p>This allows you to have your application built and executed in a container whenever any changes are made without the developer having to restart it manually making it a very nice and slick development experience.</p>
<p>One issue I’ve noticed while using this with Python was that I wasn’t getting any output from the container and since it’s a development environment that’s often what you want. Turns out the issue in my case was simply that Python output is buffered by default. Normally you would run Docker with the <code>-it</code> flag to have a pseudo-TTY attached and this debuffers the output, but this will interfere with entr. The solution is simply to pass the <code>-u</code> flag to Python in the Dockerfile disabling the default buffering. The above command then works as a charm!</p>
<p>I hope you found this post useful. If you have any questions or comments - please let me know. I’d also like to thank the author of entr, Eric Radman, for his feedback and help with making this post as useful and accurate as possible.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>AstonHack 2016 and DrumHub</title>
    <link href="https://gkudelis.net/posts/2018-01-12-astonhack-drumhub.html" />
    <id>https://gkudelis.net/posts/2018-01-12-astonhack-drumhub.html</id>
    <published>2018-01-12T00:00:00Z</published>
    <updated>2018-01-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>AstonHack 2016 and DrumHub</h2>

    <p class="info">
        Published: 12 Jan, 2018
        
        <br>Tags: <a href="/tags/webaudio.html">webaudio</a>, <a href="/tags/github.html">github</a>
    </p>

    <p>More than a year ago I went along to AstonHack 2016 as part of the Majestic team. We were mainly there to support the students trying to use Majestic API, but were pretty happy to assist them with whatever tech they were having trouble with. It was reasonably busy at first, but soon I found myself thinking I should probably start working on one of my ideas since I’m there anyway and everybody seemed to be busy with their hacks.</p>
<p>Since listening to a talk about WebAudio API during Hackference 2016 I’ve had this idea of taking a git repository and using the commit timings to create a drum track. I wanted the rhythm to speed up when the commit frequency was high and vice versa. Instead of trying to figure out the commit frequency I decided that every period between two commits would become one 4/4 measure and I would vary the tempo according to period between two adjacent commits. The other feature I wanted to add is somehow distinguishing the notes that fall on BPM peaks and troffs (e.g. if the rhythm is slowing down and after a certain point starts picking up I want the sample at that point to be different). I ended up picking a crash cymbal to denote the peaks and a tom-tom drum for the troffs.</p>
<p>Luckily for me GitHub has a <a href="https://developer.github.com/v3/">REST API for their repositories</a>, so I didn’t need to do any git parsing myself. It also meant I could build the application without any server-side code and host it using <a href="https://pages.github.com/">GitHub Pages</a>. It turns out I could get all the data using one request to the API (asking for a list of commits). The only thing left to do there was transforming the data into a time series. This was the first challenge as GitHub gives two timestamps - one as the “commiter” timestamp and one as the “author” one. This difference emerges from the fact that the original author of the commit might not have write access to the repository, in which case someone (the commiter) can take their commits and apply them. However, when multiple commits are applied this way the commiter timestamp of these commits is identical between the commits even if the author commits have different timestamps.</p>
<p>The GitHub API returns commits ordered by the commiter timestamp (as this is the order they appear in the repository). However, because of the duplication of timestamps it didn’t really fit my purpose. Of course, I could’ve removed the duplicate timestamps, but I felt that wouldn’t have been the best representation of the activity on the repository. Instead, I decided I’d use the author timestamps and simply sort them, which worked really well.</p>
<p>I knew that I wanted to map the period between commits into reasonable BPM as otherwise playing one repository could take months if not years. The mapping imposed a limit on the maximum BPM so that the shortest period between commits in any repository would correspond to one measure played at 480 beats per minute. When commits were spaced out more the tempo would decay exponentially. While testing I realised that many repositories are updated quite sporadically and in bursts - there were long periods of very low freaquency, so I added a limit to the lowest tempo and chose it to be slower by no more than a factor of 10. The resulting function for scaling is then</p>
<p><img src="/images/astonhack-drumhub/bpm-formula.png" class="formula"></p>
<p>where <var>A</var> is the smallest time period used for one measure (4 beats at 480 BPM or 0.5 s), <var>v</var> is the factor limiting the lowest tempo (in this case 10), <var>t<sub>min</sub></var> is the smallest period between commits and <var>t</var> is the period between the current commit and the previous one.</p>
<p>One value that was quite difficult to get right in the above formula was the value of <var>k</var> - the constant that controls the exponential decay or in our case how easily the output BPM drops to the lowest value. After a good amount of experimentation and listening to what different repositories sounded like I settled to a value of <var>k</var> = 0.00001 s<sup>-1</sup>.</p>
<p>Once I had the commit data and scaled it appropriately the only thing left was to actually schedule the samples and press play. I used <a href="https://www.html5rocks.com/en/tutorials/audio/scheduling://www.html5rocks.com/en/tutorials/webaudio/intro/">this intro to Web Audio API</a> by Boris Smus to figure out how to get the browser to do what I wanted. I also threw together a simple UI using <a href="https://getbootstrap.com/">Bootstrap</a> and a photo from <a href="https://unsplash.com/">Unsplash</a>.</p>
<p>You can find the hosted result at <a href="http://gkudelis.net/drumhub">gkudelis.net/drumhub</a> and the code at the <a href="https://github.com/gkudelis/drumhub">GitHub repo</a>. Pick a GitHub repository, put the username/reponame in the text field and press play. If you’re not sure what repository to try I suggest starting with tensorflow/tensorflow.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Dotfiles in git</title>
    <link href="https://gkudelis.net/posts/2016-03-06-dotfiles-in-git.html" />
    <id>https://gkudelis.net/posts/2016-03-06-dotfiles-in-git.html</id>
    <published>2016-03-06T00:00:00Z</published>
    <updated>2016-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Dotfiles in git</h2>

    <p class="info">
        Published:  6 Mar, 2016
        
        <br>Tags: <a href="/tags/git.html">git</a>
    </p>

    <p>TL;DR: I put my dotfiles on GitHub and wrote a <code>Makefile</code> that installs it. You can find it all in <a href="https://github.com/gkudelis/dotfiles">gkudelis/dotfiles</a>.</p>
<p>I recently started working for a new company. They’re using <a href="https://www.vagrantup.com/">Vagrant</a> to set up development environments and one of my first tasks was to sort out some package management issues. Long story short, I found myself starting a bunch of new machines and next week I’ll be starting many more.</p>
<p>Until now my process for getting my dotfiles onto a new machine involved a USB stick, but it’s definitely not the way to go if you need to do it more than once a month. I decided to put my dotfiles on <a href="https://github.com/">GitHub</a> and sort out some sort of installation script to make it all easier. Ideally I’d just clone the repository, <code>cd</code> into it, run something like <code>make install</code> and have all my stuff there.</p>
<p>Currently I only care about my dotfiles for <code>vim</code>, <code>tmux</code>, <code>zsh</code> and <code>git</code>. I could just symlink the <code>tmux</code>, <code>zsh</code> and <code>git</code> files, but <code>vim</code> would need something that could download all the packages. After a quick look I decided it’s time to switch to using <a href="https://github.com/VundleVim/Vundle.vim">Vundle</a> as you just have to list all the packages you need in your <code>.vimrc</code> and Vundle will install/update them for you. The process becomes:</p>
<ul>
<li>symlink <code>.vimrc</code>,</li>
<li><code>git clone</code> Vundle inside <code>.vim/bundle</code>,</li>
<li>run <code>vim +PluginInstall +qall</code> to make Vundle download and install all the plugins.</li>
</ul>
<p>After doing that I noticed that the last step was making <code>vim</code> error because of a missing colorscheme (which was about to be installed). Changing <code>colorscheme solarized</code> to <code>silent! colorscheme solarized</code> fixed that and I ended up with a non-interactive installation process.</p>
<p>After sorting out installation I realised that because I’m dealing with a number of different machines (OSX on my laptop, Linux on the work Vagrant machines) the dotfiles need to be slightly different (mostly differences in paths). My solution was to keep a branch for each of the different environments and have a master branch where changes to be applied to all branches would go. The specific branches can then be rebased onto the last commit of the master branch to bring in those general changes.</p>
<p>P.S. The morning after setting this up I found <a href="https://dotfiles.github.io/">GitHub does dotfiles</a>. If you’re interested in setting this up for yourself you should go read that first.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Getting Google Analytics</title>
    <link href="https://gkudelis.net/posts/2015-12-08-google-analytics.html" />
    <id>https://gkudelis.net/posts/2015-12-08-google-analytics.html</id>
    <published>2015-12-08T00:00:00Z</published>
    <updated>2015-12-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Getting Google Analytics</h2>

    <p class="info">
        Published:  8 Dec, 2015
        
        <br>Tags: <a href="/tags/analytics.html">analytics</a>
    </p>

    <p>Today I figured I’d like to have some idea if people actually ever visit this place and if so - why. Google Analytics can give you visibility of your users and gives a breakdown of how people found your site. It’s also really easy to set it up, so I decided to go for it.</p>
<p>I set up a new Google Analytics account, pointed it to this site, copied the JS script provided and pasted in the bottom of the base template. Little did I know that it’s supposed to be inside <code>&lt;head&gt;</code> tags… After refreshing the site a bunch of times, waiting for about an hour, refreshing again and still not seeing anything in the real-time reporting area I was told what to do. It turns out it says that in the documentation as well, but not on the page you copy the JS code from.</p>
<p>Pretty much immediately after that everything started working and I can now see all the traffic. Or lack thereof.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>Log monitor using socat, rtail, nginx and supervisor</title>
    <link href="https://gkudelis.net/posts/2015-12-03-log-monitor.html" />
    <id>https://gkudelis.net/posts/2015-12-03-log-monitor.html</id>
    <published>2015-12-03T00:00:00Z</published>
    <updated>2015-12-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>Log monitor using socat, rtail, nginx and supervisor</h2>

    <p class="info">
        Published:  3 Dec, 2015
        
        <br>Tags: <a href="/tags/logging.html">logging</a>, <a href="/tags/monitoring.html">monitoring</a>, <a href="/tags/socat.html">socat</a>, <a href="/tags/rtail.html">rtail</a>, <a href="/tags/supervisor.html">supervisor</a>, <a href="/tags/nginx.html">nginx</a>
    </p>

    <h3 id="intro">Intro</h3>
<p>Working as a developer I find myself spending a significant amount of time debugging software. And for debugging web backend software the web server logs can be an incredibly useful tool. For a while I’ve been monitoring my server logs using something like <code>tail -F /var/log/(something)/error_log</code> on the server. However, this is not ideal as I often end up with a bunch of terminal tabs monitoring different logs and swearing every time my connection drops and ssh decides to time out. This could be solved with a combination of <code>autossh</code> and <code>tmux</code>, but I wanted to be able to search the logs and filter using regular expressions. Even more importantly, I wanted to share the power of logs with my colleagues who might not be as enthusiastic about a bunch of terminal tools.</p>
<p>After finding <code>rtail</code> I realised it covers most of my requirements. It has a client which takes input from STDIN and sends it via network to the <code>rtail</code> server. The server collects the log streams coming from the different clients and serves all of them via HTTP. It allows users to switch between the streams, filter using regex and some other useful stuff.</p>
<p>My problem with using strainght <code>rtail</code> was the lack of security. The authors say you should use it behind a VPN, but that’s not something I wanted to do. Instead, I decided I’m going to place it behind an <code>nginx</code> reverse proxy to handle authentication and use <code>socat</code> to wrap the UDP packages into an SSL connection as the logs travel between my servers. I ended up using <code>supervisor</code> to make all of these into services that are started at boot and restarted in case of crashes.</p>
<h3 id="setting-up-socat">Setting up <code>socat</code></h3>
<p>The first thing to get working was <code>socat</code>. Once I know the connection works I should be able to get <code>rtail</code> using it. There’s a guide that I followed for creating an SSL connection: <a href="http://www.dest-unreach.org/socat/doc/socat-openssltunnel.html">Securing Traffic Between two Socat Instances Using SSL</a>. The only difference was that I did not want to use SSL for client authentication. That allows me to skip having to generate certificates for every client.</p>
<p>First step is generating the server key and certificate:</p>
<pre><code>openssl genrsa -out rtail-server.key 1024</code></pre>
<p>Next create a self-signed certificate:</p>
<pre><code>openssl req -new -key rtail-server.key -x509 -days 3653 -out rtail-server.crt</code></pre>
<p>You can safely ignore all the prompts by hitting enter. The next step is generating the .pem file:</p>
<pre><code>cat rtail-server.key rtail-server.crt &gt;rtail-server.pem</code></pre>
<p>The .key and .pem files must remain secret. It’s a good idea to make them only readable by you:</p>
<pre><code>chmod 600 rtail-server.key rtail-server.pem</code></pre>
<p>The .pem file is used on the <code>rtail</code> server and the .crt is used by every <code>rtail</code> client. Make sure you use something secure (like <code>scp</code>) when moving the .pem file to the server!</p>
<p>Now we can set up the SSL connection between the two servers and make sure everything works. In my case it did not, turns out there was a firewall blocking the port I wanted to use. After sorting out the firewall everything went smoothly. For the sake of argument let’s say that we’ll be using port 5333 for the SSL connection.</p>
<p>First you have to set up the server end of the SSL connection:</p>
<pre><code>socat OPENSSL-LISTEN:5333,fork,reuseaddr,cert=/path/to/srv-rtail.pem,verify=0 STDIO</code></pre>
<p>Here we’re telling <code>socat</code> we want it to fork for every new client (which means we can send multiple streams to it at the same time), to authenticate itself using the .pem file we generated previously and to not authenticate clients. We connect the stream to STDIO for debugging purposes, but later it will be pointing at the local <code>rtail</code> server. On the client side we run the client version:</p>
<pre><code>socat STDIO OPENSSL:my.rtail.server.com:5333,cafile=/path/to/srv-rtail.crt</code></pre>
<p>We’re making <code>socat</code> listen to the STDIO on the client side (which will later be listening to the output of <code>rtail</code> client applications on that server) and stream it to our <code>rtail</code> server via SSL. It will be using the .crt file to authenticate the server. If everything is correct you should be able to type things in on the client side and see them turn up on the server side.</p>
<h3 id="setting-up-rtail">Setting up <code>rtail</code></h3>
<p>Now we can get <code>rtail</code> using the SSL connection. If <code>rtail</code> had an option to use STDIO we could just pipe it to/from <code>socat</code>. However, <code>rtail</code> clients communicate with the server via UDP packets - we need to make <code>socat</code> listen to these packets on the client end and reproduce them on the server end. The only change to the above <code>socat</code> setup we replace STDIO with the corresponding UDP setup. Let’s say we’re using the port 5700 for <code>rtail</code> UDP communication. On the server this becomes:</p>
<pre><code>socat OPENSSL-LISTEN:5333,fork,reuseaddr,cert=/path/to/srv-rtail.pem,verify=0 UDP:localhost:5700</code></pre>
<p>And on the client side:</p>
<pre><code>socat UDP-LISTEN:5700 OPENSSL:my.rtail.server.com:5333,cafile=/path/to/srv-rtail.crt</code></pre>
<p>To set up <code>rtail</code> we need to run the client pointing it to the 5333 UDP port:</p>
<pre><code>rtail --port 5700 --id my-stream-name</code></pre>
<p>and on the server:</p>
<pre><code>rtail-server --web-port 5700 --udp-port 5700</code></pre>
<p>The id on the client side simply helps identify the stream on the web interface. At this point if you point your browser to the port 5700 of the <code>rtail</code> server you should see the <code>rtail</code> web interface. Test by typing things into the input stream of the rtail client - it should show up in the web interface under the stream called “my-stream-name”. If you can’t see the web interface or nothing comes through double check the firewall settings.</p>
<p>You can now pipe your log files into the <code>rtail</code> client from <code>tail</code>:</p>
<pre><code>tail -F /some/log/file | rtail --port 5700 --id some-log-file-stream</code></pre>
<h3 id="seting-up-supervisor">Seting up <code>supervisor</code></h3>
<p>The first time setting this up I just left all the <code>rtail</code> and <code>socat</code> instances running in my <code>tmux</code> sessions on various servers. Needless to say that’s not sustainable. The easiest solution I’ve found was to set up <code>supervisor</code> to manage these processes.</p>
<p>On the client side we need to run the <code>socat</code> instance and as many <code>rtail</code> clients as we need. For one client the configuration would look something like this:</p>
<pre><code>[program:rtail-client-socat-relay]
command = socat UDP-LISTEN:5700 OPENSSL:my.rtail.server.com:5333,cafile=/path/to/srv-rtail.crt
autostart = true
autorestart = true
user = nobody
priority = 900

[program:some-stream-rtail]
command = /bin/bash -c &quot;tail -F /some/stream/log | /usr/bin/rtail --port 5700 --id some-stream --mute&quot;
killasgroup = true
autostart = true
autorestart = true
user = some_stream_owner</code></pre>
<p>Since <code>socat</code> needs to authenticate the server make sure the user <code>nobody</code> has read access to the certificate. In addition to that, the <code>some_stream_owner</code> user must have read access to the log file. The priority setting defines the order in which the processes should be started with the lower-numbered ones being started first. The default is 999, so by setting the <code>socat</code> connection priority to 900 we make it start before <code>rtail</code>.</p>
<p>On the server side we open the SSL connection and start the <code>rtail</code> server:</p>
<pre><code>[program:rtail-server]
command = /usr/local/bin/rtail-server --web-port 5700 --udp-port 5700
autostart = true
autorestart = true
user = nobody

[program:rtail-server-socat-relay]
command = /usr/bin/socat OPENSSL-LISTEN:5333,fork,reuseaddr,cert=/path/to/srv-rtail.pem,verify=0 UDP:localhost:5700
autostart = true
autorestart = true
user = cert_owner</code></pre>
<p>The <code>cert_owner</code> must have read access to the certificate. The <code>fork</code> flag for <code>socat</code> makes it fork when new connection request is received, create a new connection and forward the content to the same UDP port <code>rtail</code> is listening on. The <code>verify=0</code> flag tells <code>socat</code> that the client certificates don’t need to be verified.</p>
<p>If this does not work first make sure running the same commands manually works, then check permissions and have a look at the log file produced by <code>supervisor</code>. It contains details of which processes have started successfully and can help debug the configuration.</p>
<h3 id="setting-up-nginx">Setting up <code>nginx</code></h3>
<p>The last part is setting up <code>nginx</code> as a reverse proxy with simple HTTP authentication. We tell <code>nginx</code> about the <code>rtail</code> web service we have running, then ask for all requests coming to that domain to be redirected to the rtail service subject to authentication. Here’s a good manual on how to generate the auth file: <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-http-authentication-with-nginx-on-ubuntu-12-10">How To Set Up HTTP Authentication With Nginx On Ubuntu 12.10</a>. Below is the configuration I ended up with. There is nothing but the auth file in the <code>/avr/www/mydomain.com/rtail/</code> folder.</p>
<pre><code>upstream rtail {
    server 127.0.0.1:5700;
}

server {
    server_name rtail.mydomain.com;

    auth_basic &quot;My rtail service&quot;;
    auth_basic_user_file /var/www/mydomain.com/rtail/auth;

    location / {
        proxy_pass http://rtail;
        proxy_redirect off;
    }

}</code></pre>
<h3 id="final-remarks">Final remarks</h3>
<p>One thing I’d like to see (and quite possibly get involved in myself) is creating an option that allows the <code>rtail</code> client to send its data using standard output. There’s really no need to use the UDP stack if we’re just piping the data through <code>socat</code> anyway. On the server side the <code>socat</code> instances could dump their data into a named pipe that the <code>rtail</code> server would be listening to. I think this would be a cleaner solution.</p>

</div>
]]></summary>
</entry>
<entry>
    <title>First post</title>
    <link href="https://gkudelis.net/posts/2015-11-03-first-post.html" />
    <id>https://gkudelis.net/posts/2015-11-03-first-post.html</id>
    <published>2015-11-03T00:00:00Z</published>
    <updated>2015-11-03T00:00:00Z</updated>
    <summary type="html"><![CDATA[<div class="post">

    <h2>First post</h2>

    <p class="info">
        Published:  3 Nov, 2015
        
        <br>Tags: <a href="/tags/other.html">other</a>
    </p>

    <p>I’ve been feeling I should set up a blog for a while now. I do odd projects from time to time and before this I had no place to document them. Well, while staying at Budapest I decided I should finally sort this out.</p>
<p>I wanted to make it using some sort of static pages generator as I don’t think it needs any kind of functionality really and if it will I should be able to get away with javascript. I picked hakyll to do static pages with as I was learning haskell and hey, why not? It’s been a bit of a pain, but I got the basic stuff running.</p>
<p>The whole thing is on github.io as it seems to be a good platform for this sort of thing.</p>
<p>I’m already thinking what to post about next and there’s a backlog of projects I should write up, so this will start expanding soon.</p>

</div>
]]></summary>
</entry>

</feed>
